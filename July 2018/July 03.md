# Machine Learning

## 손실 줄이기

손실을 최소화하는 매개변수 찾기

가중치와 편향에 대한 도함수 (y-y')^2는 주어진 예제의 손실 변화 정도를 보여준다.

손실을 최소화하는 방향으로 작은 보폭을 반복화여 취함.
- 기울기 보폭(음의 기울기 보폭)
- a.k.a 경사하강법

가중치 초기화  

볼록 문제에서는 가중치가 임의값을 가질 수 있다.(e.g. 모두 0)
- 볼록: 그릇 모양
- 최소값은 단 하나

예고(foreshadowing): 신경망에 해당 안됨  
- 볼록하지 않음: 계란판 모양
- 최소값 둘 이상 존재
- 초기값에 따라 크게 달라짐

SGD와 미니 배치 경사하강법

각 보폭마다 전체 데이터 세트에 대해 기울기를 계산할 수 있지만 그보다 적은 양의 데이터 샘플로도 충분히 괜찮은 결과를 얻을 수 있음이 경험적으로 입증되었다.  

확률적 경사하강법: 한 번에 하나의 예  
미니 배치 경사하강법: 10~1000개의 예로 구성된 배치

## 반복 방식
최적의 모델을 가능한 한 가장 효율적으로 찾는 것

피쳐들 -> 모델(예측 함수) -> 손실 계산 -> 매개변수 업데이트 계산 -> 모델(예측 함수)
라벨 -> 손실 계산

주로 대규모 데이터 세트에 적용하기 용이함.  
이 '모델'은 하나 이상의 특성을 입력하여 하나의 예측(y')을 출력한다.

e.g. 하나의 특성을 가지고 하나의 예측을 반환하는 모델

y' = b + w1x1  
b = 0, w1 = 0

최초 특성값 10이라 가정.
y' = 0 + 0(10)  
y' = 0

'손실 계산'과정에서 제곱 손실 함수를 사용한다고 가정하면 두 개의 입력 값을 취한다.

y': 특성 x에 대한 모델의 예측 값
y: 특성 x에 대한 올바른 라벨

'매개변수 업데이트 계산'에서 머신러닝은 손실 함수의 값을 검토하여 b와 w1의 새로운 값을 생성한다.  
이후 머신러닝이 이러한 모든 특성을 모든 라벨과 대조하여 재평가하여 손실 함수의 새로운 값을 생성하여 새 매개변수 값을 출력한다고 가정한다.  
알고리즘이 손실 값이 가장 낮은 모델 배개변수를 발견할때까지 반복학습한다.  
전체 손실이 변하지 않거나 매우 느리게 변할 때까지 계속 반복한다.(모델 수렴)

## 경사하강법

w1에 가능한 모든 값에 대해 손실을 계산할 시간과 컴퓨팅 자료가 있다고 가정한다. 지금까지 살펴본 회귀 무제에서 손실/가중치 wi를 대응한 도표는 항상 볼록 함수 모양이다.  
볼록 문제에는 기울기가 정확하게 0인 지점인 최소값이 하나만 존재하고, 손실 함수가 수렴하게 된다.

이 수렴 지점을 찾는데 널리 쓰이는 방법이 경사하강법이다.

1. w1에 대한 시작 값 선택
    - 0 또는 임의의 값
2. 시작점에서 손실 곡선의 기울기 계산(편미분의 벡터)
3. 기울기의 크기의 일부를 시작점에 합산
4. 기울기 보폭(학습률)을 통해 다음 지점으로 이동

## 학습률

경사하강법 알고리즘은 기울기에 학습률(보폭)이라 불리는 스칼라를 곱하여 다음 지점을 결정한다.  
e.g. 기울기 = 2.5, 학습률 0.01  
-> 이전 지점으로부터 0.025 떨어진 지점

초매개변수: 프로그래머가 머신러닝 알고리즘에서 조정하는 값  
대부분의 머신러닝 프로그래머는 학습률을 미세조정하는데 상당한 시간을 소비한다.

학습률이 너무 작으면 학습 시간이 매우 오래 걸리고, 너무 크면 최소값을 찾지 못하고 이탈할 가능성이 있다.

골디락스 값은 손실 함수가 얼마나 평탄한지와 관련이 있다. 손실 함수의 기울기가 작다면 더 큰 학습률을 시도해 볼 수 있다.

1차원의 경우 이상적인 학습률은 x에서 f(x)의 2계도함수의 역이다.

2차원 이상의 경우 이상적인 학습률은 Hessian 행렬(2계편미분의 행렬)의 역이다.

## 확률적 경사하강법

배치: 단일 반복에서 기울기를 계산하는 데 사용하는 예의 총 개수, 즉 데이터 세트

보통 데이터 세트는 매우 많은 특성이 포함되어 있기 때문에 단일 반복으로도 오랜 시간이 걸린다.

확률적 경사하강법(SGD)은 반복당 하나의 예만을 사용한다. 반복이 충분하면 SGD가 효과는 있지만 노이즈가 매우 심하다. 확률적(stochastic)이라는 용어는 각 배치를 포함한는 하나의 예가 무작위로 선택됨을 의미한다.

미니 배치 확률적 경사하강법은 전체 배치 반복과 SGD 간의 절충안이다.